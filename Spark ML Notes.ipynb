{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Linear Regression with Spark", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190608195437-0000\nKERNEL_ID = eef0faa2-24b2-4f65-a547-2bb6cb624e11\n"
                }
            ], 
            "source": "import ibmos2spark"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "configuration_name = 'os_45b77029797b4322b732418ac53a105b_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedatasci-donotdelete-pr-bvizfkp0o2ej2y'))\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.createOrReplaceTempView('df')"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_energy = spark.sql(\"\"\"\n\nselect sqrt(sum(X*X)+sum(Y*Y)+sum(Z*Z)) as label, class from df group by class\n\n\"\"\")\n\ndf_energy.createOrReplaceTempView('df_energy')"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+-----------------+-----------+\n|  x|  y|  z|      class|              source|            label|      class|\n+---+---+---+-----------+--------------------+-----------------+-----------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|\n+---+---+---+-----------+--------------------+-----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df_join = spark.sql(\"\"\"\n\nselect * from df inner join df_energy on df.class = df_energy.class\n\n\"\"\")\n\ndf_join.show()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols = [\"x\", \"y\", \"z\"],\n                                 outputCol = \"features\")\nnormalizer = Normalizer(inputCol = \"features\", outputCol = \"features_norm\", p = 1.0)"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.regression import LinearRegression"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lr = LinearRegression(maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages = [vectorAssembler, normalizer, lr])"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df_join)"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_join)"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\n|  x|  y|  z|      class|              source|            label|      class|        features|       features_norm|        prediction|\n+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,36.0]|[0.23008849557522...|11260.914132330869|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,36.0]|[0.23008849557522...|11260.914132330869|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,37.0]|[0.22807017543859...|11332.369820764736|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[25.0,51.0,37.0]|[0.22123893805309...| 11313.99441223891|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,37.0]|[0.22807017543859...|11332.369820764736|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[27.0,51.0,37.0]|[0.23478260869565...|11350.745229290562|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,37.0]|[0.22807017543859...|11332.369820764736|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[23.0,51.0,38.0]|[0.20535714285714...|11348.699283621123|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[25.0,51.0,39.0]|[0.21739130434782...|11456.905789106642|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,51.0,39.0]|[0.22413793103448...| 11475.28119763247|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[25.0,52.0,39.0]|[0.21551724137931...| 11463.53040623109|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[25.0,50.0,38.0]|[0.22123893805309...|11378.825483548328|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[26.0,49.0,39.0]|[0.22807017543859...|11462.031963383573|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[24.0,48.0,39.0]|[0.21621621621621...|11418.656529207474|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[24.0,47.0,39.0]|[0.21818181818181...|11412.031912083025|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[23.0,47.0,39.0]|[0.21100917431192...|11393.656503557198|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[20.0,48.0,39.0]|[0.18691588785046...|11345.154895104168|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[21.0,47.0,40.0]|[0.19444444444444...|11428.361374939414|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[21.0,47.0,42.0]|[0.19090909090909...|11571.272751807146|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|11785.39634462923|Brush_teeth|[19.0,47.0,43.0]|[0.17431192660550...| 11605.97762318936|\n+---+---+---+-----------+--------------------+-----------------+-----------+----------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "prediction.show()"
        }, 
        {
            "source": "# Logistic Regression with Spark", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "configuration_name = 'os_45b77029797b4322b732418ac53a105b_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedatasci-donotdelete-pr-bvizfkp0o2ej2y'))\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n\nvectorAssembler = VectorAssembler(inputCols = [\"x\", \"y\", \"z\"],\n                                 outputCol = \"features\")\n\nnormalizer = Normalizer(inputCol = \"features\", outputCol = \"features_norm\", p = 1.0)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import LogisticRegression"
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lr = LogisticRegression(maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)"
        }, 
        {
            "execution_count": 62, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline"
        }, 
        {
            "execution_count": 63, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages = [indexer, vectorAssembler, normalizer, lr])"
        }, 
        {
            "execution_count": 64, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df_train)"
        }, 
        {
            "execution_count": 65, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_train)"
        }, 
        {
            "execution_count": 66, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
        }, 
        {
            "execution_count": 67, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "eval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')"
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 69, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.7915369679785497"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "eval.evaluate(prediction)"
        }, 
        {
            "execution_count": 70, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df_test)"
        }, 
        {
            "execution_count": 71, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_test)"
        }, 
        {
            "execution_count": 72, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 72, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.7158023184868822"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "eval.evaluate(prediction)"
        }, 
        {
            "source": "# SVM Using Apache SparkML", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 73, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "configuration_name = 'os_45b77029797b4322b732418ac53a105b_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedatasci-donotdelete-pr-bvizfkp0o2ej2y'))\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()"
        }, 
        {
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer"
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "indexer = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n\nencoder = OneHotEncoder(inputCol = 'label', outputCol = 'labelVec')\n\nvectorAssembler = VectorAssembler(inputCols = [\"x\", \"y\", \"z\"],\n                                 outputCol = \"features\")\n\nnormalizer = Normalizer(inputCol = \"features\", outputCol = \"features_norm\", p = 1.0)"
        }, 
        {
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import LinearSVC"
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lsvc = LinearSVC(maxIter = 10, regParam = 0.1)"
        }, 
        {
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"select * from df where class in ('Use_telephone', 'Standup_chair')\")\nsplits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline"
        }, 
        {
            "execution_count": 80, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline(stages = [indexer, encoder, vectorAssembler, normalizer, lsvc])"
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df_train)"
        }, 
        {
            "execution_count": 82, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_train)"
        }, 
        {
            "execution_count": 83, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
        }, 
        {
            "execution_count": 84, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 84, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9386319573255747"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'rawPrediction')\nevaluator.evaluate(prediction)"
        }, 
        {
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_test)"
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 86, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9396165941798973"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "evaluator.evaluate(prediction)"
        }, 
        {
            "source": "# Gradient Boosted Trees with Apache SparkML", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 87, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "configuration_name = 'os_45b77029797b4322b732418ac53a105b_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedatasci-donotdelete-pr-bvizfkp0o2ej2y'))\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()"
        }, 
        {
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        }, 
        {
            "execution_count": 89, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"select * from df where class in ('Use_telephone', 'Standup_chair')\")\nsplits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        }, 
        {
            "execution_count": 90, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer"
        }, 
        {
            "execution_count": 91, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "indexer = StringIndexer(inputCol = \"class\", outputCol = \"label\")\n\nvectorAssembler = VectorAssembler(inputCols = [\"x\", \"y\", \"z\"],\n                                 outputCol = \"features\")\n\nnormalizer = Normalizer(inputCol = \"features\", outputCol = \"features_norm\", p = 1.0)"
        }, 
        {
            "execution_count": 92, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import GBTClassifier"
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "gbt = GBTClassifier(labelCol = 'label', featuresCol = 'features', maxIter = 10)"
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages = [indexer, vectorAssembler, normalizer, gbt])"
        }, 
        {
            "execution_count": 95, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df_train)"
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = model.transform(df_train)"
        }, 
        {
            "execution_count": 97, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 97, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9112766677954622"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"label\")\n\nbinEval.evaluate(prediction)"
        }, 
        {
            "execution_count": 98, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 98, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.8991297953180537"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "prediction = model.transform(df_test)\nbinEval.evaluate(prediction)"
        }, 
        {
            "source": "# Hyperparameter-Tuning using GridSeach and CrossValidation in Apache SparkML on Gradient Boosted Trees", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 99, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
        }, 
        {
            "execution_count": 100, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "paramGrid = ParamGridBuilder() \\\n    .addGrid(normalizer.p, [1.0, 2.0, 10.0]) \\\n    .addGrid(gbt.maxBins, [2, 4, 8, 16]) \\\n    .addGrid(gbt.maxDepth, [2, 4, 8, 16]) \\\n    .build()"
        }, 
        {
            "execution_count": 101, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "crossval = CrossValidator(estimator = pipeline,\n                         estimatorParamMaps = paramGrid,\n                         evaluator = MulticlassClassificationEvaluator(),\n                         numFolds = 4)"
        }, 
        {
            "execution_count": 102, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "KeyboardInterrupt", 
                    "evalue": "", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m/opt/ibm/conda/miniconda36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-102-f3a34bb6a313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/conda/miniconda36/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/ibm/conda/miniconda36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "cvModel = crossval.fit(df_train)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "prediction = cvModel.transform(df_test)\nbinEval.evaluate(prediction)"
        }, 
        {
            "source": "# Using K-Means in Apache SparkML", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 103, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 36|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 25| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 27| 51| 37|Brush_teeth|Accelerometer-201...|\n| 26| 51| 37|Brush_teeth|Accelerometer-201...|\n| 23| 51| 38|Brush_teeth|Accelerometer-201...|\n| 25| 51| 39|Brush_teeth|Accelerometer-201...|\n| 26| 51| 39|Brush_teeth|Accelerometer-201...|\n| 25| 52| 39|Brush_teeth|Accelerometer-201...|\n| 25| 50| 38|Brush_teeth|Accelerometer-201...|\n| 26| 49| 39|Brush_teeth|Accelerometer-201...|\n| 24| 48| 39|Brush_teeth|Accelerometer-201...|\n| 24| 47| 39|Brush_teeth|Accelerometer-201...|\n| 23| 47| 39|Brush_teeth|Accelerometer-201...|\n| 20| 48| 39|Brush_teeth|Accelerometer-201...|\n| 21| 47| 40|Brush_teeth|Accelerometer-201...|\n| 21| 47| 42|Brush_teeth|Accelerometer-201...|\n| 19| 47| 43|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "configuration_name = 'os_45b77029797b4322b732418ac53a105b_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'advancedatasci-donotdelete-pr-bvizfkp0o2ej2y'))\n\ndf.createOrReplaceTempView(\"df\")\nspark.sql(\"SELECT * from df\").show()"
        }, 
        {
            "execution_count": 104, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler(inputCols = [\"x\", \"y\", \"z\"],\n                                 outputCol = \"features\")"
        }, 
        {
            "execution_count": 105, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.clustering import KMeans"
        }, 
        {
            "execution_count": 106, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "kmeans = KMeans().setK(13).setSeed(1)"
        }, 
        {
            "execution_count": 107, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages = [vectorAssembler, kmeans])"
        }, 
        {
            "execution_count": 108, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df)"
        }, 
        {
            "execution_count": 109, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "13737484.905328393\n"
                }
            ], 
            "source": "wssse = model.stages[1].computeCost(vectorAssembler.transform(df))\nprint(wssse)"
        }, 
        {
            "execution_count": 110, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.createOrReplaceTempView('df')\ndf = spark.sql(\"select * from df where class in ('Climb_stairs', 'Brush_teeth')\")"
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = pipeline.fit(df)"
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "2618507.395322505\n"
                }
            ], 
            "source": "wssse = model.stages[1].computeCost(vectorAssembler.transform(df))\nprint(wssse)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}