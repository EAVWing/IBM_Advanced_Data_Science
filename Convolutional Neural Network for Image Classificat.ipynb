{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n60000 train samples\n10000 test samples\nWARNING:tensorflow:From /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               262656    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 669,706\nTrainable params: 669,706\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/20\n60000/60000 [==============================] - 13s 209us/step - loss: 0.2461 - acc: 0.9228 - val_loss: 0.1126 - val_acc: 0.9649\nEpoch 2/20\n60000/60000 [==============================] - 12s 198us/step - loss: 0.1028 - acc: 0.9685 - val_loss: 0.1065 - val_acc: 0.9661\nEpoch 3/20\n60000/60000 [==============================] - 12s 201us/step - loss: 0.0761 - acc: 0.9770 - val_loss: 0.0681 - val_acc: 0.9797\nEpoch 4/20\n60000/60000 [==============================] - 12s 196us/step - loss: 0.0608 - acc: 0.9816 - val_loss: 0.0805 - val_acc: 0.9778\nEpoch 5/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0481 - acc: 0.9854 - val_loss: 0.0685 - val_acc: 0.9820\nEpoch 6/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0448 - acc: 0.9863 - val_loss: 0.0767 - val_acc: 0.9812\nEpoch 7/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0379 - acc: 0.9886 - val_loss: 0.0740 - val_acc: 0.9842\nEpoch 8/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0332 - acc: 0.9894 - val_loss: 0.0817 - val_acc: 0.9813\nEpoch 9/20\n60000/60000 [==============================] - 12s 200us/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0783 - val_acc: 0.9831\nEpoch 10/20\n60000/60000 [==============================] - 12s 200us/step - loss: 0.0283 - acc: 0.9916 - val_loss: 0.0815 - val_acc: 0.9838\nEpoch 11/20\n60000/60000 [==============================] - 12s 201us/step - loss: 0.0260 - acc: 0.9923 - val_loss: 0.0939 - val_acc: 0.9821\nEpoch 12/20\n60000/60000 [==============================] - 12s 200us/step - loss: 0.0254 - acc: 0.9929 - val_loss: 0.0848 - val_acc: 0.9833\nEpoch 13/20\n60000/60000 [==============================] - 12s 197us/step - loss: 0.0233 - acc: 0.9934 - val_loss: 0.1031 - val_acc: 0.9825\nEpoch 14/20\n60000/60000 [==============================] - 12s 197us/step - loss: 0.0229 - acc: 0.9939 - val_loss: 0.0858 - val_acc: 0.9835\nEpoch 15/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.0997 - val_acc: 0.9836\nEpoch 16/20\n60000/60000 [==============================] - 12s 198us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0846 - val_acc: 0.9843\nEpoch 17/20\n60000/60000 [==============================] - 12s 198us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0950 - val_acc: 0.9832\nEpoch 18/20\n60000/60000 [==============================] - 12s 200us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.1054 - val_acc: 0.9834\nEpoch 19/20\n60000/60000 [==============================] - 12s 201us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.1028 - val_acc: 0.9845\nEpoch 20/20\n60000/60000 [==============================] - 12s 199us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 0.1030 - val_acc: 0.9835\nTest loss: 0.10298656079\nTest accuracy: 0.9835\n"
                }
            ], 
            "source": "'''Trains a simple deep NN on the MNIST dataset.\n\nGets to 98.40% test accuracy after 20 epochs\n(there is *a lot* of margin for parameter tuning).\n2 seconds per epoch on a K520 GPU.\n'''\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.optimizers import RMSprop\nfrom keras.layers import Conv2D, MaxPooling2D\n\nbatch_size = 128\nnum_classes = 10\nepochs = 20\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Please construct the following neural network and report accuracy after training\n#Layer 1: 2D Convolution with 32 hidden neurons, kernel 3 by 3, relu activation, input_shape (28,28,1)\n#Layer 2: 2D MaxPooling, pool_size 2 by 2\n#Layer 3: Flatten (Hint: model.add(Flatten()))\n#Layer 4 Softmax Output Layer according to the problem (output vector)\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n#input dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "60000 train samples\n10000 test samples\n(28, 28, 1)\n"
                }
            ], 
            "source": "#import keras backend\nfrom keras import backend as K\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n    \nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\nprint(input_shape)\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<keras.engine.sequential.Sequential at 0x7fad5c444278>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model2 = Sequential()\nmodel2.add(Conv2D(32, kernel_size=(3,3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25))\nmodel2.add(Flatten())\nmodel2.add(Dense(128, activation='relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(num_classes, activation='softmax'))\n\nmodel2.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nmodel2"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 285s 5ms/step - loss: 2.3725 - acc: 0.1487 - val_loss: 2.0426 - val_acc: 0.1955\nEpoch 2/12\n60000/60000 [==============================] - 274s 5ms/step - loss: 6.1125 - acc: 0.1272 - val_loss: 14.4894 - val_acc: 0.1010\nEpoch 3/12\n60000/60000 [==============================] - 274s 5ms/step - loss: 8.3971 - acc: 0.1132 - val_loss: 14.4902 - val_acc: 0.1010\nEpoch 4/12\n60000/60000 [==============================] - 275s 5ms/step - loss: 8.3998 - acc: 0.1196 - val_loss: 14.6791 - val_acc: 0.0892\nEpoch 5/12\n60000/60000 [==============================] - 276s 5ms/step - loss: 8.4506 - acc: 0.1296 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 6/12\n60000/60000 [==============================] - 276s 5ms/step - loss: 8.6795 - acc: 0.1319 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 7/12\n60000/60000 [==============================] - 277s 5ms/step - loss: 8.6130 - acc: 0.1377 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 8/12\n60000/60000 [==============================] - 276s 5ms/step - loss: 8.6395 - acc: 0.1399 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 9/12\n60000/60000 [==============================] - 272s 5ms/step - loss: 8.7301 - acc: 0.1357 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 10/12\n60000/60000 [==============================] - 273s 5ms/step - loss: 8.6652 - acc: 0.1405 - val_loss: 14.6804 - val_acc: 0.0892\nEpoch 11/12\n60000/60000 [==============================] - 276s 5ms/step - loss: 8.5765 - acc: 0.1411 - val_loss: 14.6802 - val_acc: 0.0892\nEpoch 12/12\n60000/60000 [==============================] - 277s 5ms/step - loss: 8.5169 - acc: 0.1550 - val_loss: 14.4892 - val_acc: 0.1010\nTest loss: 14.4891867477\nTest accuracy: 0.101\n"
                }
            ], 
            "source": "history2 = model2.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\nscore = model2.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-06-12 22:43:16--  https://vignette.wikia.nocookie.net/liberapedia/images/b/bc/Elephant.jpg/revision/latest?cb=20130413020301\nResolving vignette.wikia.nocookie.net (vignette.wikia.nocookie.net)... 74.120.184.194, 74.120.184.204, 2620:11a:e00e:fa00::194, ...\nConnecting to vignette.wikia.nocookie.net (vignette.wikia.nocookie.net)|74.120.184.194|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 86706 (85K) [image/jpeg]\nSaving to: \u2018latest?cb=20130413020301\u2019\n\n100%[======================================>] 86,706      --.-K/s   in 0.07s   \n\n2019-06-12 22:43:16 (1.12 MB/s) - \u2018latest?cb=20130413020301\u2019 saved [86706/86706]\n\n"
                }
            ], 
            "source": "!wget https://vignette.wikia.nocookie.net/liberapedia/images/b/bc/Elephant.jpg/revision/latest?cb=20130413020301"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!mv latest?cb=20130413020301 elephant.jpg"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n102858752/102853048 [==============================] - 1s 0us/step\n"
                }, 
                {
                    "execution_count": 19, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(224, 224, 3)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx.shape"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = np.expand_dims(x, axis=0)\nx = preprocess_input(x)"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 21, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(1, 224, 224, 3)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "x.shape"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "preds = model.predict(x)"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 23, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[  1.95510133e-08,   1.67362280e-09,   2.14313829e-08,\n          6.46397291e-09,   1.54294995e-08,   3.78236990e-07,\n          1.47317451e-07,   8.90222296e-09,   1.05635034e-09,\n          3.01986773e-07,   1.35542899e-09,   1.41499068e-09,\n          2.33135955e-09,   6.69250610e-09,   1.02109032e-09,\n          2.99274272e-09,   1.45844687e-08,   5.07665021e-09,\n          1.02417266e-08,   1.98832928e-09,   1.84202498e-09,\n          4.89193859e-08,   1.55898956e-08,   1.08922950e-06,\n          9.97374450e-09,   5.21330223e-09,   1.06221414e-08,\n          3.00421092e-08,   1.85769400e-09,   1.85139082e-09,\n          6.92796598e-10,   1.70730274e-09,   2.49001197e-09,\n          2.55170153e-06,   3.83120050e-06,   3.65773403e-08,\n          1.91083245e-07,   3.48239446e-08,   3.01145008e-07,\n          3.07481787e-06,   1.35905127e-08,   5.01928881e-08,\n          3.44563637e-06,   1.54745942e-06,   6.64799344e-08,\n          2.74915919e-07,   1.63505369e-08,   3.92498077e-06,\n          4.14618717e-06,   1.73807152e-07,   1.28704301e-08,\n          5.41119443e-05,   4.81451412e-09,   6.40292575e-09,\n          5.91607439e-08,   5.88673821e-09,   1.22587469e-08,\n          1.63537106e-09,   7.35103711e-09,   7.26180716e-09,\n          1.42870844e-08,   2.78671930e-09,   1.21291537e-08,\n          6.90603272e-08,   3.66479433e-08,   6.68745628e-08,\n          2.92593228e-07,   3.74735407e-08,   3.79022538e-08,\n          5.72234114e-07,   1.43591994e-09,   8.23095405e-08,\n          1.54476609e-09,   4.06754141e-09,   1.62206892e-09,\n          6.06802208e-09,   1.91957295e-08,   9.76933201e-09,\n          1.07680753e-08,   1.38909868e-07,   2.94446423e-08,\n          7.20562499e-09,   4.48102817e-08,   2.02993036e-08,\n          7.11609571e-09,   8.04692704e-08,   2.41541954e-07,\n          2.10679659e-07,   5.79223780e-08,   3.00113401e-09,\n          2.33148412e-09,   3.73805982e-08,   1.74796462e-08,\n          1.19238805e-08,   5.39569056e-10,   4.11172041e-10,\n          1.69828362e-09,   4.42351000e-09,   6.08392225e-09,\n          4.86384799e-09,   1.33245992e-08,   6.64068907e-02,\n          1.55913142e-07,   4.23994528e-09,   2.34083810e-08,\n          1.53699613e-08,   2.25688098e-07,   4.76176680e-08,\n          6.70775213e-09,   5.39077121e-07,   9.97760807e-09,\n          2.31604353e-08,   3.70784363e-07,   4.27510933e-08,\n          8.43630499e-09,   4.29592628e-09,   2.84507728e-09,\n          1.74649823e-08,   9.51012424e-08,   9.75321299e-08,\n          1.49960513e-07,   2.12364739e-08,   7.35141592e-09,\n          2.05592396e-08,   1.84087469e-07,   2.63238348e-07,\n          2.93293994e-08,   1.23924195e-08,   2.18465779e-08,\n          8.57394689e-09,   4.93622254e-09,   1.63613016e-08,\n          4.64394967e-09,   4.46558257e-09,   2.54392347e-08,\n          3.72638320e-09,   5.74121017e-09,   4.68567158e-08,\n          2.86047054e-07,   2.24520491e-09,   2.23983676e-09,\n          4.76359929e-09,   3.12999093e-09,   6.10408879e-09,\n          1.81744646e-08,   7.77539100e-08,   4.52786040e-08,\n          1.91264018e-08,   7.16880333e-09,   1.51422896e-07,\n          1.74352266e-08,   6.47809317e-09,   1.75662684e-09,\n          4.80131099e-08,   3.68144022e-08,   1.54653570e-08,\n          2.36546516e-08,   3.81661502e-09,   2.83506800e-08,\n          1.62930789e-07,   9.71511511e-08,   8.58819078e-08,\n          4.52470914e-08,   4.68366110e-08,   2.89232513e-08,\n          4.07592360e-08,   8.52865334e-09,   9.84326753e-09,\n          4.34074323e-08,   9.05133390e-09,   6.89788351e-08,\n          1.77514217e-08,   2.98839211e-08,   2.06008881e-08,\n          3.10110657e-08,   1.80205131e-08,   1.15259153e-08,\n          1.38318113e-08,   7.45882119e-07,   5.22561976e-08,\n          5.80059591e-08,   7.59136753e-07,   3.24843832e-08,\n          5.20195442e-07,   7.99584186e-08,   2.16775593e-08,\n          5.09056726e-08,   1.22146586e-08,   1.31548514e-08,\n          6.55278285e-08,   9.04681698e-08,   7.79783278e-08,\n          1.15360983e-08,   1.71180350e-08,   2.96498204e-09,\n          1.33684296e-07,   3.22774838e-08,   4.90920513e-07,\n          1.72559211e-07,   2.15673204e-08,   3.52446534e-08,\n          2.01848671e-08,   6.83393466e-07,   7.21709910e-08,\n          1.48015449e-08,   7.80695473e-08,   1.32114337e-07,\n          1.54599817e-07,   3.53581271e-07,   3.06407060e-07,\n          2.00616999e-08,   1.64090480e-07,   1.03538982e-08,\n          1.24551205e-08,   1.10088777e-07,   3.86179728e-08,\n          1.34202139e-07,   1.05750857e-07,   6.66552458e-09,\n          4.17067945e-08,   1.07891687e-08,   8.77629361e-07,\n          1.16759857e-07,   4.42200232e-08,   4.21571507e-08,\n          2.34113280e-08,   1.65423501e-07,   4.87656052e-07,\n          1.21968895e-07,   2.83951010e-07,   3.13864916e-08,\n          1.31011973e-07,   2.88586115e-08,   1.88757497e-07,\n          4.39702575e-07,   4.71514063e-08,   4.29742784e-08,\n          5.06257436e-08,   1.61836695e-07,   4.05781897e-08,\n          2.53466794e-08,   1.11708154e-07,   6.65679991e-08,\n          1.47805906e-07,   7.77604399e-08,   1.43043650e-07,\n          1.60534555e-07,   1.51630761e-08,   2.79277064e-08,\n          1.95204308e-07,   6.81836667e-08,   1.41783559e-07,\n          9.77704850e-09,   8.06013389e-09,   1.35290179e-07,\n          3.02840881e-08,   4.41033876e-07,   1.45473464e-07,\n          3.54895064e-07,   2.29329107e-08,   3.22390747e-07,\n          1.51428168e-08,   1.38314675e-08,   2.48699941e-08,\n          1.17221477e-08,   9.61037205e-09,   4.70684718e-08,\n          7.30157751e-07,   5.07474851e-07,   1.29497991e-07,\n          2.72188458e-07,   2.83445711e-08,   7.87283909e-08,\n          1.20032723e-07,   7.44712914e-09,   7.94993227e-08,\n          1.71756525e-07,   6.19307912e-08,   1.71736954e-08,\n          9.90041471e-09,   4.02130382e-07,   3.78350791e-08,\n          2.20339285e-08,   3.62228270e-09,   6.90604240e-09,\n          8.21818489e-08,   1.65215397e-07,   4.30062599e-08,\n          2.23819381e-08,   1.23716537e-09,   4.25620108e-08,\n          3.78280987e-07,   9.05030291e-08,   1.09005619e-07,\n          1.63676225e-06,   7.98663962e-07,   4.50599919e-06,\n          7.59351792e-07,   2.14683064e-06,   5.32878737e-08,\n          2.29665265e-08,   1.24068693e-08,   1.31679556e-08,\n          1.05962608e-08,   4.84692819e-09,   1.27213593e-06,\n          1.18425794e-08,   1.45106336e-08,   2.99818770e-09,\n          2.91825164e-09,   7.14034130e-08,   1.17560237e-08,\n          1.36113956e-08,   8.93231586e-08,   5.24079162e-08,\n          1.04062448e-08,   2.69104605e-09,   8.78231265e-09,\n          4.05167588e-09,   1.79942294e-09,   3.47254314e-09,\n          1.29860780e-08,   6.33148360e-08,   1.17445653e-09,\n          9.33371602e-10,   7.48660689e-09,   1.77349990e-09,\n          2.37860476e-07,   2.12797076e-07,   3.88064088e-08,\n          9.32388122e-09,   3.75548792e-09,   5.45956658e-09,\n          2.30374586e-09,   1.45710047e-07,   5.86438054e-09,\n          1.20936548e-07,   3.48881479e-09,   1.20735271e-08,\n          1.76595933e-07,   1.75840762e-06,   1.29684190e-06,\n          4.97942119e-06,   1.12380385e-05,   1.41468661e-06,\n          7.00674718e-07,   6.86346721e-06,   2.19210233e-06,\n          2.44100011e-06,   5.73818909e-07,   1.82687458e-07,\n          1.12494142e-07,   5.83942601e-08,   1.48327814e-07,\n          4.45183177e-05,   2.29025417e-07,   7.05740959e-08,\n          2.09540598e-08,   5.33204059e-08,   2.11401279e-08,\n          5.43069412e-09,   4.96014998e-08,   2.67880949e-07,\n          4.09964559e-06,   2.31966855e-08,   1.95948871e-07,\n          6.10250027e-07,   1.66621746e-07,   2.48324046e-08,\n          6.23226226e-09,   5.84301389e-08,   1.40417072e-07,\n          6.49048297e-06,   7.38254329e-08,   1.91369853e-08,\n          7.33536432e-09,   1.76473769e-08,   4.52770221e-09,\n          7.46644258e-09,   8.26999180e-09,   9.42841005e-09,\n          8.95802277e-09,   2.44632261e-08,   1.08924096e-08,\n          2.09407158e-08,   1.75863765e-02,   9.15722728e-01,\n          4.54897053e-09,   1.31916869e-08,   9.19411747e-09,\n          3.15812336e-08,   1.38893377e-08,   3.22971268e-08,\n          5.45536150e-09,   1.15364953e-08,   1.30396472e-07,\n          1.80857320e-08,   2.43010625e-08,   2.20136287e-09,\n          1.95951316e-07,   1.82061677e-09,   6.31259756e-09,\n          3.44429907e-08,   5.05672115e-10,   4.74827777e-09,\n          3.66653516e-08,   3.99424316e-09,   1.12544257e-07,\n          5.86357878e-08,   1.11250529e-08,   2.31117037e-09,\n          5.78709969e-09,   1.43423469e-08,   3.49632114e-06,\n          3.53776066e-08,   9.85368898e-10,   1.98252792e-09,\n          1.99978984e-08,   1.44682124e-08,   1.22462268e-07,\n          2.29090382e-08,   1.48876182e-08,   5.67203973e-09,\n          4.35366765e-09,   3.25843996e-09,   5.21991161e-08,\n          2.67961546e-08,   7.19316347e-07,   1.62080426e-07,\n          1.45991862e-07,   1.32551625e-09,   2.02262580e-08,\n          8.61585292e-08,   2.04129179e-07,   3.31881182e-07,\n          2.15921915e-07,   1.18688561e-08,   1.31188456e-08,\n          1.86292866e-08,   1.11106488e-07,   1.67458811e-08,\n          8.06993228e-09,   4.76190287e-08,   1.18930581e-08,\n          2.09342375e-07,   5.67873251e-08,   1.51301425e-08,\n          2.41249381e-07,   1.52640567e-08,   1.29643247e-08,\n          3.40935546e-09,   2.70003966e-08,   1.04006844e-07,\n          1.30094318e-08,   2.32251862e-09,   3.31784311e-09,\n          2.02623909e-07,   1.16299773e-07,   1.05955685e-07,\n          1.34512534e-08,   4.73388404e-08,   4.10477412e-08,\n          1.28686210e-07,   1.48698825e-06,   4.95723711e-08,\n          1.24202884e-07,   5.36130307e-10,   1.23348087e-08,\n          9.92378180e-09,   9.45120426e-09,   1.42625805e-08,\n          1.68041041e-07,   2.04851929e-07,   3.43108049e-08,\n          2.48585411e-06,   2.51633931e-08,   5.25613275e-10,\n          3.86964238e-08,   2.11712504e-07,   5.64747609e-07,\n          3.50759919e-08,   8.66748895e-09,   4.27302016e-09,\n          1.97279704e-08,   7.05138135e-08,   2.21341678e-09,\n          1.17673080e-08,   1.10293223e-08,   8.61802292e-08,\n          1.39872505e-08,   5.07946743e-06,   1.51770962e-06,\n          1.92097946e-08,   1.38407321e-08,   7.40825357e-09,\n          1.72847925e-09,   2.04148853e-09,   3.28511263e-08,\n          7.54131158e-09,   3.69424207e-08,   7.56175353e-08,\n          2.52970409e-07,   2.72203131e-08,   2.08279372e-09,\n          3.38928388e-08,   7.14366166e-09,   2.11525908e-08,\n          4.80609152e-09,   4.19430890e-08,   1.51007273e-09,\n          5.21474419e-09,   1.60244241e-07,   1.26247199e-08,\n          1.24001032e-08,   2.33905865e-07,   6.26926976e-07,\n          8.30743332e-08,   1.21709380e-08,   3.20998822e-07,\n          2.56375348e-08,   1.01317168e-08,   1.03458888e-07,\n          8.03394862e-09,   2.94110745e-08,   7.66163339e-07,\n          1.17595444e-08,   2.04692867e-08,   2.72289724e-09,\n          1.76544823e-08,   1.54038077e-07,   2.27314967e-08,\n          4.32977654e-09,   2.50317509e-08,   2.60046864e-08,\n          1.07891950e-07,   1.03444027e-08,   8.37608294e-09,\n          1.03731317e-08,   5.65371359e-08,   1.39363408e-07,\n          8.43208969e-09,   5.29354161e-09,   7.08517645e-09,\n          3.37382353e-08,   8.04898015e-09,   7.03911240e-08,\n          1.33204747e-09,   1.02425612e-09,   2.41116171e-09,\n          1.22984902e-08,   2.49610821e-09,   9.24816916e-08,\n          3.46104621e-08,   6.19268903e-08,   4.75234607e-09,\n          1.75043677e-08,   6.69965816e-09,   5.35882450e-08,\n          1.33597933e-08,   1.34578233e-08,   1.28576900e-07,\n          1.13125820e-08,   8.12606160e-09,   5.97368421e-09,\n          1.10045839e-09,   1.99648547e-08,   1.51983137e-08,\n          3.89624333e-09,   4.27237666e-07,   1.69827719e-09,\n          2.99079147e-07,   3.18004254e-07,   1.93075311e-09,\n          9.56963286e-10,   5.52320589e-09,   1.24650779e-08,\n          2.51033944e-10,   8.87790819e-09,   3.93186284e-08,\n          8.43718961e-09,   4.20068735e-09,   1.92344700e-08,\n          1.02268709e-08,   3.11979909e-09,   3.70639386e-09,\n          1.49636286e-08,   6.20285476e-08,   1.01587545e-07,\n          1.59848298e-07,   2.33992736e-08,   2.40938269e-09,\n          3.74308975e-08,   4.20735091e-09,   1.89476115e-08,\n          9.47266354e-09,   1.05678978e-07,   4.30547885e-07,\n          4.36946181e-07,   1.24853505e-08,   3.42019497e-08,\n          9.45578904e-08,   1.50693165e-08,   4.15003010e-09,\n          2.64790060e-07,   4.39575487e-09,   3.10493000e-08,\n          8.25135338e-09,   2.96985103e-08,   4.67970324e-07,\n          2.98925602e-06,   1.57022740e-07,   2.50767176e-08,\n          8.23316810e-08,   1.86434335e-08,   3.96636821e-08,\n          1.70802764e-07,   7.02107812e-08,   1.33510811e-08,\n          2.56272537e-09,   8.24322441e-08,   3.53046019e-08,\n          3.63417385e-08,   2.69054965e-08,   1.49284496e-08,\n          3.80248943e-09,   1.34028504e-08,   5.03740338e-09,\n          9.47370626e-10,   2.52855359e-09,   3.16644555e-09,\n          3.36672663e-08,   1.11072849e-08,   5.07088327e-09,\n          2.09365218e-08,   7.20094022e-07,   8.71287220e-09,\n          2.90330888e-07,   9.08685109e-08,   5.35587390e-07,\n          2.46071409e-07,   1.94146352e-08,   3.47384810e-09,\n          2.03414334e-08,   2.01720898e-08,   3.65991637e-09,\n          1.34956024e-09,   2.18028280e-08,   7.63802532e-09,\n          1.31727536e-08,   1.03232446e-06,   4.96368955e-08,\n          3.61640282e-08,   2.06086520e-06,   1.99608969e-08,\n          1.28360043e-07,   1.45534372e-07,   2.86775954e-08,\n          1.46688265e-07,   1.75130506e-08,   2.10148485e-07,\n          1.67161058e-08,   3.05099546e-09,   6.47148601e-09,\n          2.95402138e-08,   1.22633770e-08,   3.17495008e-08,\n          5.21287724e-08,   6.92067914e-09,   1.20725607e-08,\n          1.06935283e-09,   4.57420448e-08,   1.30973760e-07,\n          2.54853720e-08,   1.61312865e-08,   3.86075882e-08,\n          1.15225740e-08,   1.31159572e-06,   1.08143297e-07,\n          5.41686553e-08,   4.43651382e-09,   4.91764363e-09,\n          4.17235029e-08,   7.76789051e-08,   1.58809197e-08,\n          5.57909896e-09,   2.75306578e-09,   5.53145298e-08,\n          6.21826035e-10,   3.95902688e-09,   2.99114049e-08,\n          9.90876998e-08,   5.18169117e-08,   1.15192520e-07,\n          2.34246844e-08,   4.60196947e-09,   9.20500085e-08,\n          3.41748887e-08,   4.31082618e-08,   4.35524790e-08,\n          1.75549886e-08,   1.05704878e-07,   2.34956961e-08,\n          3.85252585e-09,   1.07929921e-07,   1.62489702e-08,\n          4.83648366e-09,   2.24103882e-08,   4.10071266e-08,\n          4.73277346e-08,   2.52375116e-08,   1.62990066e-08,\n          2.54567922e-09,   3.82255960e-09,   2.57977408e-08,\n          1.15640111e-08,   1.53773883e-07,   1.05816582e-08,\n          2.62944269e-07,   3.63092418e-08,   5.91654832e-08,\n          2.36480058e-09,   2.22251764e-08,   1.00796989e-07,\n          3.72543774e-09,   2.82441377e-08,   1.36053480e-08,\n          7.81873766e-09,   1.23311628e-08,   1.04222210e-07,\n          1.75164914e-08,   1.24619885e-06,   3.82670535e-08,\n          1.75350117e-08,   6.18932319e-08,   1.55940576e-08,\n          1.02867013e-07,   3.16275162e-09,   1.28128947e-08,\n          2.09929965e-07,   4.19908766e-08,   3.68066075e-08,\n          5.21182351e-08,   2.81775794e-08,   6.33033679e-08,\n          3.17073670e-07,   8.37660963e-09,   6.68426470e-09,\n          1.13315641e-08,   9.18994481e-09,   4.99358999e-09,\n          1.05247395e-07,   4.75127919e-08,   2.63046491e-08,\n          3.11862252e-08,   1.31075399e-08,   1.42341259e-08,\n          8.64374897e-07,   4.59237910e-08,   1.10648681e-08,\n          1.61965161e-07,   3.74087875e-07,   1.07252145e-08,\n          3.26212124e-09,   2.51708325e-08,   1.74928550e-06,\n          4.29500091e-07,   9.85126536e-10,   2.64636721e-08,\n          3.74505849e-09,   3.63813371e-08,   2.01824797e-08,\n          2.70586540e-08,   4.21667437e-07,   6.86199986e-10,\n          1.89774781e-07,   7.21773219e-08,   9.63047029e-08,\n          7.64155175e-07,   1.95761518e-09,   5.55988278e-09,\n          2.48348684e-08,   1.47804624e-09,   2.30772272e-08,\n          1.49502853e-07,   4.10002450e-08,   1.63653269e-08,\n          3.57927803e-08,   2.15341586e-07,   9.59733004e-09,\n          2.99241911e-08,   1.98805363e-08,   3.23313358e-08,\n          1.02005333e-06,   5.12901579e-08,   6.70712836e-08,\n          1.46341259e-08,   4.37025953e-08,   2.94869604e-08,\n          1.04094013e-08,   3.12337285e-08,   2.99931857e-10,\n          5.57269964e-08,   1.93331164e-08,   1.58661457e-08,\n          1.82818951e-08,   3.33403875e-07,   2.74417076e-08,\n          1.00359408e-07,   3.12412226e-07,   6.01716144e-09,\n          7.93813992e-09,   1.38109852e-08,   2.23859082e-08,\n          1.77065491e-08,   5.25662625e-09,   9.55209423e-09,\n          6.36598885e-10,   3.21596016e-09,   2.27048051e-08,\n          4.06929557e-09,   1.73925461e-08,   1.19918013e-08,\n          6.92182756e-09,   3.18297779e-08,   3.96908497e-08,\n          1.20876820e-07,   9.65442926e-09,   2.44965186e-08,\n          2.02432886e-07,   5.31234223e-10,   1.40004303e-07,\n          6.24218757e-08,   4.74509356e-08,   5.76837644e-09,\n          8.32673493e-07,   3.04633332e-08,   1.06880229e-07,\n          3.56941996e-08,   2.78903389e-08,   2.85898150e-09,\n          2.82059290e-07,   3.13533604e-07,   1.88189779e-06,\n          5.59035094e-08,   6.64921131e-08,   8.81155859e-09,\n          2.27829755e-08,   2.67789261e-07,   1.84321625e-09,\n          2.66840883e-09,   2.05339848e-06,   3.14588888e-08,\n          3.82263430e-08,   1.33888489e-06,   2.70776379e-09,\n          1.36109550e-08,   1.64677417e-07,   5.72835779e-09,\n          4.02253534e-07,   7.54125384e-09,   1.71496606e-09,\n          3.85582837e-08,   8.72113493e-09,   1.16802337e-08,\n          7.13040205e-09,   6.77585632e-08,   7.87862220e-08,\n          1.81618507e-08,   1.40178236e-09,   2.08446494e-07,\n          5.28854898e-08,   1.87453768e-08,   1.21025153e-07,\n          1.85098195e-08,   5.85026749e-10,   1.28294264e-08,\n          9.73463330e-07,   6.11112538e-09,   5.82326276e-09,\n          1.26562256e-07,   1.52313632e-07,   8.81774564e-09,\n          9.95144234e-08,   1.25984281e-08,   5.84549937e-08,\n          6.50835190e-08,   2.46942378e-09,   5.07525622e-09,\n          5.98288619e-09,   1.96469618e-09,   4.86964025e-09,\n          4.05372447e-09,   2.04080042e-08,   3.51565674e-08,\n          7.02244041e-09,   1.59304406e-07,   3.64617136e-08,\n          1.14625136e-08,   6.64267006e-08,   1.17376239e-07,\n          4.80688236e-08,   6.88794088e-09,   2.37835867e-08,\n          1.38561829e-08,   9.75942349e-09,   1.22738841e-08,\n          2.33027162e-08,   4.19957580e-08,   4.23497459e-09,\n          5.28997868e-09,   5.30520872e-08,   6.43841702e-07,\n          5.35692379e-08,   6.04953868e-07,   3.30133587e-09,\n          3.74618239e-08,   2.93551441e-08,   1.25032473e-08,\n          1.31287328e-08,   2.93028961e-08,   2.72718492e-09,\n          1.16654952e-08,   1.73430470e-09,   6.24161256e-09,\n          6.68245503e-09,   1.54590154e-08,   1.01479192e-09,\n          3.31245587e-09,   1.18961196e-08,   1.18029305e-07,\n          2.11602984e-08,   1.32942377e-08,   7.05585768e-09,\n          1.54102864e-09,   1.01602806e-08,   5.95100813e-09,\n          5.81955373e-09,   5.10579525e-08,   4.19138999e-08,\n          2.06720507e-08,   5.74047654e-09,   3.81078813e-09,\n          2.36911344e-08,   4.19831068e-08,   6.08251804e-09,\n          5.06687048e-09,   6.30672448e-09,   1.11550811e-07,\n          4.36938219e-09,   8.81122286e-09,   3.97472135e-08,\n          6.85130574e-09,   1.15922116e-08,   1.22483694e-08,\n          2.88482660e-08,   1.72044892e-07,   1.55499735e-09,\n          7.08127246e-09,   1.53320229e-07,   6.17863405e-09,\n          6.07213257e-09,   1.13514815e-07,   6.74001921e-09,\n          2.14670060e-09,   5.45013012e-09,   8.89780583e-10,\n          8.78846329e-09,   2.53986432e-09,   9.14912412e-09,\n          1.20287458e-09,   4.44085941e-08,   6.75605989e-08,\n          7.50052891e-07,   4.37537437e-07,   5.75501602e-08,\n          3.59972141e-08,   2.93629995e-07,   8.72613964e-06,\n          6.66002222e-07,   5.48661427e-09,   1.33855806e-08,\n          6.60673436e-08,   1.09314598e-07,   1.09326272e-07,\n          2.58092072e-08,   4.76299977e-09,   3.64748232e-10,\n          1.17163417e-07,   3.66308028e-09,   6.34312443e-08,\n          4.08957268e-09,   1.24984787e-08,   4.06637835e-09,\n          4.21654134e-09,   7.09059789e-09,   3.19095328e-08,\n          7.45450457e-09,   1.24285888e-08,   3.73777453e-08,\n          2.19676686e-08]], dtype=float32)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "preds"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n40960/35363 [==================================] - 0s 0us/step\nPredicted: [('n02504458', 'African_elephant', 0.91572273), ('n01871265', 'tusker', 0.066406891), ('n02504013', 'Indian_elephant', 0.017586377)]\n"
                }
            ], 
            "source": "# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}